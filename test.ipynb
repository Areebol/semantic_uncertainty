{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/semantic_uncertainty/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.56s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer,AutoModelForCausalLM\n",
    "\n",
    "cache_dir = \"/U_20240603_ZSH_SMIL/LLM/models--meta-llama--Llama-2-7b-hf/snapshots/01c7f73d771dfac7d292323805ebc428287df4f9\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "                    cache_dir, device_map=\"auto\",\n",
    "                    token_type_ids=None)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "                        cache_dir, device_map=\"auto\",\n",
    "                        max_memory={0: '80GIB'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import StoppingCriteria\n",
    "from transformers import StoppingCriteriaList\n",
    "import torch\n",
    "class StoppingCriteriaSub(StoppingCriteria):\n",
    "    \"\"\"Stop generations when they match a particular text or token.\"\"\"\n",
    "    def __init__(self, stops, tokenizer, match_on='text', initial_length=None):\n",
    "        super().__init__()\n",
    "        self.stops = stops\n",
    "        self.initial_length = initial_length\n",
    "        self.tokenizer = tokenizer\n",
    "        self.match_on = match_on\n",
    "        if self.match_on == 'tokens':\n",
    "            self.stops = [torch.tensor(self.tokenizer.encode(i)).to('cuda') for i in self.stops]\n",
    "            print(self.stops)\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor):\n",
    "        del scores  # `scores` arg is required by StoppingCriteria but unused by us.\n",
    "        for stop in self.stops:\n",
    "            if self.match_on == 'text':\n",
    "                generation = self.tokenizer.decode(input_ids[0][self.initial_length:], skip_special_tokens=False)\n",
    "                match = stop in generation\n",
    "            elif self.match_on == 'tokens':\n",
    "                # Can be dangerous due to tokenizer ambiguities.\n",
    "                match = stop in input_ids[0][-len(stop):]\n",
    "            else:\n",
    "                raise\n",
    "            if match:\n",
    "                return True\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer([\"Answer the following question as briefly as possible.\\nQuestion: Which group recorded the 1976 album 'Rastaman Vibration'?\\nAnswer: wailers\\n\\nQuestion: Which car company produces the Meriva model?\\nAnswer: vauxhall\\n\\nQuestion: Who directed the first two Beatles' films 'A Hard Day's Night' and 'Help! '?\\nAnswer: richard lester\\n\\nQuestion: Which of the 'Classic' horse races, run at Epsom for three year old fillies on the Friday after the derby, is named after the estate then owned by the Earl of Derby?\\nAnswer: oaks\\n\\nQuestion: In which country is the most northerly point on mainland Africa?\\nAnswer: tunisia\\n\\nQuestion: Who is the host of the BBC television show QI?\\nAnswer:\"], return_tensors=\"pt\").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/semantic_uncertainty/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `10.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/envs/semantic_uncertainty/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:397: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/envs/semantic_uncertainty/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:407: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.6817, -0.0199, -0.0930, -0.0012, -0.0251, -0.0437, -0.0224, -0.0008]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "stopping_criteria = StoppingCriteriaList([StoppingCriteriaSub(\n",
    "    stops=['Question:'],\n",
    "    initial_length=len(inputs['input_ids'][0]),\n",
    "    tokenizer=tokenizer)])\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        # num_beams=4,\n",
    "        \n",
    "        # num_return_sequences=4,\n",
    "        \n",
    "        max_new_tokens=50,\n",
    "        return_dict_in_generate=True,\n",
    "        output_scores=True,\n",
    "        output_hidden_states=True,\n",
    "        top_k=20,\n",
    "        temperature=10.0,\n",
    "        do_sample=False,\n",
    "        stopping_criteria=stopping_criteria,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "    )\n",
    "    \n",
    "transition_scores = model.compute_transition_scores(\n",
    "    outputs.sequences, outputs.scores, normalize_logits=True\n",
    "    # outputs.sequences, outputs.scores, outputs.beam_indices, normalize_logits=False\n",
    ")\n",
    "print(transition_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "tensor([-12.5141,  -9.4003,  -9.4026,  -9.4805], dtype=torch.float64)\n",
      "tensor([-0.1110, -0.2870, -0.3838, -0.4125], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "output_length = np.sum(transition_scores.cpu().numpy() < 0, axis=1)\n",
    "length_penalty = model.generation_config.length_penalty\n",
    "reconstructed_scores = transition_scores.cpu().sum(axis=1) / (output_length**length_penalty)\n",
    "print(np.allclose(outputs.sequences_scores.cpu(), reconstructed_scores))\n",
    "print(reconstructed_scores)\n",
    "print(outputs.sequences_scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "semantic_uncertainty",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
